{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATOR\n",
      "-----------\n",
      "(u'inputs:0', [None, 256, 513, 1])\n",
      "(u'generator/Relu:0', [None, 256, 513, 32])\n",
      "(u'generator/add_14:0', [None, 256, 513, 32])\n",
      "(u'generator/add_15:0', [None, 256, 513, 32])\n",
      "(u'generator/Relu_16:0', [None, 256, 513, 128])\n",
      "(u'generator/Relu_17:0', [None, 256, 513, 128])\n",
      "(u'generator/gconv3/Tanh:0', [None, 256, 513, 1])\n",
      "(u'inputs:0', [None, 256, 513, 1])\n",
      "(u'generator/Relu_18:0', [None, 256, 513, 32])\n",
      "(u'generator/add_30:0', [None, 256, 513, 32])\n",
      "(u'generator/add_31:0', [None, 256, 513, 32])\n",
      "(u'generator/Relu_34:0', [None, 256, 513, 128])\n",
      "(u'generator/Relu_35:0', [None, 256, 513, 128])\n",
      "(u'generator/gconv3_1/Tanh:0', [None, 256, 513, 1])\n",
      "initialization done\n",
      "data train: 117\n",
      "data test: 45\n",
      "TRAINING\n",
      "-----------\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Dst tensor is not initialized.\n\t [[Node: _arg_real_ims_0_1/_3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_10350__arg_real_ims_0_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Mean_3/_521 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_10352_Mean_3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0ff20fdee702>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_optim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0merrG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_ims\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_origs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Dst tensor is not initialized.\n\t [[Node: _arg_real_ims_0_1/_3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_10350__arg_real_ims_0_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Mean_3/_521 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_10352_Mean_3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from analyzer import *\n",
    "import time\n",
    "import pdb\n",
    "\n",
    "def print_shape(t):\n",
    "    print(t.name, t.get_shape().as_list())\n",
    "\n",
    "def optimistic_restore(session, save_file, graph=tf.get_default_graph()):\n",
    "    reader = tf.train.NewCheckpointReader(save_file)\n",
    "    saved_shapes = reader.get_variable_to_shape_map()\n",
    "    var_names = sorted([(var.name, var.name.split(':')[0]) for var in tf.global_variables()\n",
    "            if var.name.split(':')[0] in saved_shapes])    \n",
    "    restore_vars = []    \n",
    "    for var_name, saved_var_name in var_names:            \n",
    "        curr_var = graph.get_tensor_by_name(var_name)\n",
    "        var_shape = curr_var.get_shape().as_list()\n",
    "        if var_shape == saved_shapes[saved_var_name]:\n",
    "            restore_vars.append(curr_var)\n",
    "    opt_saver = tf.train.Saver(restore_vars)\n",
    "    opt_saver.restore(session, save_file)\n",
    "\n",
    "\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)\n",
    "\n",
    "def create_error_metrics(gen, inputs, origs):\n",
    "    # Losses\n",
    "\n",
    "    # metric: L2 between downsampled generated output and input\n",
    "    gen_LR = slim.avg_pool2d(gen, [4, 4], stride=1, padding='SAME')\n",
    "    gen_mse_LR = tf.reduce_mean(tf.square(tf.contrib.layers.flatten(gen_LR - inputs)), 1)\n",
    "    gen_L2_LR = tf.reduce_mean(gen_mse_LR)\n",
    "\n",
    "    # metric: L2 between generated output and the original image\n",
    "    gen_mse_HR = tf.reduce_mean(tf.square(tf.contrib.layers.flatten(gen - origs)), 1)\n",
    "    # average for the batch\n",
    "    gen_L2_HR = tf.reduce_mean(gen_mse_HR)\n",
    "\n",
    "    # metric: PSNR between generated output and original input\n",
    "    gen_rmse_HR = tf.sqrt(gen_mse_HR)\n",
    "    gen_PSNR = tf.reduce_mean(20*tf.log(1.0/gen_rmse_HR)/tf.log(tf.constant(10, dtype=tf.float32)))\n",
    "\n",
    "    err_im_HR = gen - origs\n",
    "    err_im_LR = gen_LR - inputs\n",
    "\n",
    "    return gen_L2_LR, gen_L2_HR, gen_PSNR, err_im_LR, err_im_HR\n",
    "\n",
    "\n",
    "class batch_norm(object):\n",
    "    \"\"\"Code modification of http://stackoverflow.com/a/33950177\"\"\"\n",
    "    def __init__(self, epsilon=1e-5, momentum = 0.9, name=\"batch_norm\"):\n",
    "        with tf.variable_scope(name):\n",
    "            self.epsilon = epsilon\n",
    "            self.momentum = momentum\n",
    "\n",
    "            self.ema = tf.train.ExponentialMovingAverage(decay=self.momentum)\n",
    "            self.name = name\n",
    "\n",
    "    def __call__(self, x, train=True, b_reuse=False):\n",
    "        shape = x.get_shape().as_list()\n",
    "\n",
    "        if train:\n",
    "            with tf.variable_scope(self.name) as scope:\n",
    "                self.beta = tf.get_variable(\"beta\", [shape[-1]],\n",
    "                                    initializer=tf.constant_initializer(0.))\n",
    "                self.gamma = tf.get_variable(\"gamma\", [shape[-1]],\n",
    "                                    initializer=tf.random_normal_initializer(1., 0.02))\n",
    "\n",
    "                # work around reuse=True problem\n",
    "                with tf.variable_scope(tf.get_variable_scope(), reuse=False):\n",
    "                    batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name='moments')\n",
    "                    ema_apply_op = self.ema.apply([batch_mean, batch_var])\n",
    "                    self.ema_mean, self.ema_var = self.ema.average(batch_mean), self.ema.average(batch_var)\n",
    "\n",
    "                    with tf.control_dependencies([ema_apply_op]):\n",
    "                        mean, var = tf.identity(batch_mean), tf.identity(batch_var)\n",
    "\n",
    "        else:\n",
    "            mean, var = self.ema_mean, self.ema_var\n",
    "\n",
    "        normed = tf.nn.batch_norm_with_global_normalization(\n",
    "                x, mean, var, self.beta, self.gamma, self.epsilon, scale_after_normalization=True)\n",
    "\n",
    "        return normed\n",
    "\n",
    "\n",
    "\n",
    "########\n",
    "# CONFIG \n",
    "#########\n",
    "adam_learning_rate = 0.0001\n",
    "adam_beta1 = 0.9\n",
    "\n",
    "batch_size = 32\n",
    "image_h = 256\n",
    "image_w = 513\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "###############################\n",
    "# BUILDING THE MODEL\n",
    "###############################\n",
    "\n",
    "\n",
    "real_ims = tf.placeholder(tf.float32, [None, image_h, image_w, 1], name='real_ims')\n",
    "inputs = tf.placeholder(tf.float32, [None, image_h, image_w, 1], name='inputs')\n",
    "\n",
    "\n",
    "# generator section\n",
    "print \"GENERATOR\"\n",
    "print \"-----------\"\n",
    "\n",
    "# not great way to create these\n",
    "batch_norm_list = []\n",
    "nb_residual = 15\n",
    "n_extra_bn = 1\n",
    "for n in range(nb_residual*2 + n_extra_bn):\n",
    "    batch_norm_list.append(batch_norm(name='bn'+str(n)))\n",
    "\n",
    "def create_generator(inputs, b_training=True):\n",
    "    with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n",
    "                        padding='SAME',\n",
    "                        activation_fn=None,\n",
    "                        weights_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                        weights_regularizer=slim.l2_regularizer(0.0005)):\n",
    "        net = inputs\n",
    "        print_shape(net)\n",
    "\n",
    "        net = tf.nn.relu(slim.conv2d(net, 32, [3, 3], scope='gconv1'))\n",
    "        print_shape(net)\n",
    "\n",
    "        net1 = net\n",
    "\n",
    "        res_inputs = net1\n",
    "        for n in range(nb_residual):\n",
    "            net = tf.nn.relu(batch_norm_list[n*2](slim.conv2d(res_inputs, 32, [3, 3], scope='conv1_res'+str(n)), train=b_training))\n",
    "            net = batch_norm_list[n*2+1](slim.conv2d(net, 32, [3, 3], scope='conv2_res'+str(n)), train=b_training)\n",
    "            net = net + res_inputs\n",
    "            res_inputs = net\n",
    "\n",
    "\n",
    "        print_shape(net)\n",
    "\n",
    "        net = batch_norm_list[-1](slim.conv2d(net, 32, [3, 3], scope='gconv2'), train=b_training) + net1\n",
    "        print_shape(net)\n",
    "\n",
    "        # deconv\n",
    "        net = tf.nn.relu(slim.conv2d_transpose(net, 128, [5, 5], stride=1, scope='deconv1'))\n",
    "        print_shape(net)\n",
    "\n",
    "        net = tf.nn.relu(slim.conv2d_transpose(net, 128, [5, 5], stride=1, scope='deconv2'))\n",
    "        print_shape(net)\n",
    "\n",
    "\n",
    "        # tanh since images have range [-1,1]\n",
    "        net = slim.conv2d(net, 1, [3, 3], scope='gconv3', activation_fn=tf.nn.tanh)\n",
    "        print_shape(net)\n",
    "\n",
    "    return net\n",
    "\n",
    "with tf.variable_scope(\"generator\") as scope:\n",
    "    gen = create_generator(inputs)\n",
    "    scope.reuse_variables()\n",
    "    gen_test = create_generator(inputs, False)\n",
    "\n",
    "\n",
    "gen_L2_LR, gen_L2_HR, gen_PSNR, err_im_LR, err_im_HR = create_error_metrics(gen, inputs, real_ims)\n",
    "\n",
    "# metrics for testing stream\n",
    "gen_L2_LR_t, gen_L2_HR_t, gen_PSNR_t, err_im_LR_t, err_im_HR_t = create_error_metrics(gen_test, inputs, real_ims)\n",
    "\n",
    "# baselines: L2 and PSNR between bicubic upsampled input and original image\n",
    "\n",
    "train_vars = tf.trainable_variables()\n",
    "g_vars = [var for var in train_vars if 'generator' in var.name]\n",
    "\n",
    "\n",
    "# optimize the generator and discriminator separately\n",
    "g_loss = gen_L2_HR\n",
    "\n",
    "g_optim = tf.train.AdamOptimizer(adam_learning_rate, beta1=adam_beta1) \\\n",
    "                  .minimize(g_loss, var_list=g_vars)\n",
    "    \n",
    "weight_saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "\n",
    "# logging\n",
    "g_L2HR_train     = tf.summary.scalar(\"gen_L2_HR\", gen_L2_HR)\n",
    "g_L2HR_test      = tf.summary.scalar(\"gen_L2_HR\", gen_L2_HR_t)\n",
    "\n",
    "\n",
    "merged_summary_train = tf.summary.merge([g_L2HR_train])\n",
    "merged_summary_test = tf.summary.merge([g_L2HR_test])\n",
    "\n",
    "# tf will automatically create the log directory\n",
    "train_writer = tf.summary.FileWriter('./logs_pair_supervised/train')\n",
    "test_writer = tf.summary.FileWriter('./logs_pair_supervised/test')\n",
    "\n",
    "print \"initialization done\"\n",
    "\n",
    "\n",
    "#############\n",
    "# TRAINING\n",
    "############\n",
    "\n",
    "data_dir = 'data/matrix_sample/225/pair/'\n",
    "data_train = glob.glob(data_dir+\"/train/*.bin\")\n",
    "data_test = glob.glob(data_dir+\"/test/*.bin\")\n",
    "\n",
    "print \"data train:\", len(data_train)\n",
    "print \"data test:\", len(data_test)\n",
    "\n",
    "\n",
    "# create directories to save checkpoint and samples\n",
    "samples_dir = 'samples_pair_supervised'\n",
    "if not os.path.exists(samples_dir):\n",
    "    os.makedirs(samples_dir)\n",
    "\n",
    "checkpoint_dir = 'checkpoint_pair_supervised'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "\n",
    "print \"TRAINING\"\n",
    "print \"-----------\"\n",
    "\n",
    "start_time = time.time()\n",
    "counter = 0\n",
    "\n",
    "b_load = False\n",
    "ckpt_dir = 'data/matrix_sample/225/checkpoint_pair_supervised/model-60002'\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    num_batches = len(data_train) // batch_size\n",
    "\n",
    "    if b_load:\n",
    "        # optimistic_restore(sess, ckpt_dir)\n",
    "        # print \"successfully restored!\"\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "        weight_saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        counter = int(ckpt.model_checkpoint_path.split('-', 1)[1]) \n",
    "        print \"successfully restored!\" + \" counter:\", counter\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        np.random.shuffle(data_train)\n",
    "\n",
    "        for idx in xrange(num_batches):\n",
    "            batch_filenames = data_train[idx*batch_size : (idx+1)*batch_size]\n",
    "            \n",
    "            batch_inputs, batch_origs = read_pair_batch_numpy(batch_filenames)\n",
    "            \n",
    "            # update networks\n",
    "            \n",
    "            fetches = [g_loss, g_optim]\n",
    "            errG, _ = sess.run(fetches, feed_dict={inputs:batch_inputs, real_ims:batch_origs})\n",
    "            \n",
    "\n",
    "            counter += 1\n",
    "            print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, g_loss: %.8f\" \\\n",
    "                % (epoch, idx, num_batches,\n",
    "                    time.time() - start_time, errG))\n",
    "\n",
    "#             if np.mod(counter, 30) == 1:\n",
    "\n",
    "#                 # training metrics first\n",
    "#                 train_summary = sess.run([merged_summary_train], feed_dict={inputs: batch_inputs, real_ims: batch_origs})\n",
    "#                 train_writer.add_summary(train_summary[0], counter)\n",
    "\n",
    "#                 # now testing metrics\n",
    "#                 rand_idx = np.random.randint(len(data_test)-batch_size+1)\n",
    "#                 sample_origs, sample_inputs = get_images(data_test[rand_idx: rand_idx+batch_size])\n",
    "\n",
    "#                 sample = sess.run([gen_test], feed_dict={inputs: sample_inputs})\n",
    "\n",
    "#                 err_im_HR = sess.run([err_im_HR_t], feed_dict={inputs: sample_inputs, real_ims: sample_origs})\n",
    "\n",
    "#                 test_summary = sess.run([merged_summary_test], feed_dict={ inputs: sample_inputs, real_ims: sample_origs})\n",
    "#                 test_writer.add_summary(test_summary[0], counter)\n",
    "\n",
    "#                 # save an image, with the original next to the generated one\n",
    "#                 resz_input = sample_inputs[0].repeat(axis=0,repeats=4).repeat(axis=1,repeats=4)\n",
    "#                 merge_im = np.zeros( (image_h, image_h*4, 3) )\n",
    "#                 merge_im[:, :image_h, :] = (sample_origs[0]+1)*127.5\n",
    "#                 merge_im[:, image_h:image_h*2, :] = (resz_input+1)*127.5\n",
    "#                 merge_im[:, image_h*2:image_h*3, :] = (sample[0][0]+1)*127.5\n",
    "#                 merge_im[:, image_h*3:, :] = (err_im_HR[0][0]+1)*127.5\n",
    "#                 imsave(samples_dir + '/test_{:02d}_{:04d}.png'.format(epoch, idx), merge_im)\n",
    "\n",
    "\n",
    "\n",
    "#             if np.mod(counter, 1000) == 2:\n",
    "#                 weight_saver.save(sess, checkpoint_dir + '/model', counter)\n",
    "#                 print \"saving a checkpoint\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4fe801d52b7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/train/*.bin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "data_train = glob(data_dir+\"/train/*.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/train/*.bin'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(data_dir, \"/train/*.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/matrix_sample/225/pair//train/*.bin'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir+\"/train/*.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
